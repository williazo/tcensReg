---
title: "tcensReg"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tcensReg}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Maximum Likelihood Estimation of a Truncated Normal Distribution with Censored Data

The goal of this package is to estimate parameters from a linear model when the data comes from a truncated normal distribution with censoring. Maximum likelihood values are returned derived from Newton-Raphson algorithm using analytic values of the gradient and hessian. This package is also able to return maximum likelihood estimates for truncated only or censored only data similar to `truncreg` and `censReg` packages.

## Installation
You can install ggplot.spaghetti from github via the devtools package with:

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("williazo/tcensReg")
```


## Example 1: Single Population

Some common examples where this type of problem may arise is when there is a natural truncation imposed by the structure of the data. For instance several applications have an implied zero truncation such as product lifetimes, age, or detection thresholds. To show how to implement the functions within the package, I will demonstrate a simple simulation example.

Assume that we have observations from an underlying truncated normal distribution 

$Y^{*}\sim\text{TN}(\mu, \sigma^{2}, a)$, 

where $a$ denotes the value of the left-truncation. In our case we will assume a zero-truncated model by setting $a=0$.

```{r}
library(msm) #we will use this package to generate random values from the truncated normal distribution
mu <- 0.5
sigma <- 0.5
a <- 0

y_star <- msm::rtnorm(n = 1000, mean = mu, sd = sigma, lower = a)
range(y_star) #note that the lowerbound will always be non-negative
```

Next, we can imagine a scenario where we have an imprecise measurement of $Y^{*}$ leading to censoring. In our case we assume that values below $\nu$ are censored such that $a<\nu$. This creates the random variable $Y$, where

$Y_{i}=\nu\big(1_{\{Y_{i}^{*}\le\nu\}}\big)+Y_{i}^{*}\big(1-1_{\{Y_{i}^{*}\le\nu\}}\big)$ and $1_{\{Y_{i}^{*}\le\nu\}}=1$ is $Y_{i}^{*}\le\nu$ and 0 otherwise.

In the example below we set $\nu=0.25$.
```{r}
nu <- 0.25
y <- ifelse(y_star <= nu, nu, y_star)
sum(y == nu)/length(y) #calculating the number of censored observations
dt <- data.frame(y_star, y) #collecting the uncensored and censored data together

```

We can observe the histogram and density plot for the uncensored data, which shows the zero-truncation.
```{r echo=FALSE, warning=FALSE, message = FALSE, out.width="40%", fig.align="center"}
library(ggplot2)
library(viridis)
ggplot(data = dt, aes(x = y_star, y = ..density..))+
  geom_histogram(binwidth = 0.25, col = viridis(1),fill = viridis(1), alpha = 0.6)+
  stat_density(bw = 0.5, col = viridis(1), fill = viridis(1), alpha = 0.3)+
  scale_x_continuous(breaks = seq(0, 2, 0.25))+
  ylab("Density")+
  xlab(expression(Y^"*"))+
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.grid.major = element_blank(), 
        panel.background = element_blank(), panel.border = element_rect(fill = NA, color = "black"),
        axis.text.x = element_text(size = 10), legend.title = element_text(size = 15),
        legend.text = element_text(size = 15),  legend.key.width = unit(15, units = "mm"), strip.text = element_text(size = 20),
        axis.title = element_text(size = 20))
```


We can then compare this to the censored observations below
```{r echo=FALSE, warning=FALSE, message = FALSE, out.width="40%", fig.align="center"}
library(ggplot2)
library(viridis)
ggplot(data = dt, aes(x = y, y = ..density..))+
  geom_histogram(binwidth = 0.25, col = viridis(1, begin = 0.5),fill = viridis(1, begin = 0.5), alpha = 0.6)+
  stat_density(bw = 0.25, col = viridis(1, begin = 0.5), fill = viridis(1, begin = 0.5), alpha = 0.3)+
  scale_x_continuous(breaks = seq(0, 2, 0.25))+
  ylab("Density")+
  xlab(expression(Y))+
    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.grid.major = element_blank(), 
        panel.background = element_blank(), panel.border = element_rect(fill = NA, color = "black"),
        axis.text.x = element_text(size = 10), legend.title = element_text(size = 15),
        legend.text = element_text(size = 15),  legend.key.width = unit(15, units = "mm"), strip.text = element_text(size = 20),
        axis.title = element_text(size = 20))
```

We can then estimate $\mu$ and $\sigma$ using our observed $Y$ values with the `tcensReg` package as shown below.
```{r message = FALSE}
library(tcensReg)  #loading the package into the current environment
tcensReg(y ~ 1, data = dt, a = 0, v = 0.25)
```

Note that the this will return parameter estimates, variance-covariance matrix, the number of iterations until convergence, and the initial/final log-likelihood values.

Comparing the values to the truth we see that the estimates are unbiased.
```{r message = FALSE}
output <- tcensReg(y ~ 1, data = dt, a = a, v = nu)
lm_output <- lm(y ~ 1, data = dt) #running OLS model for comparison
cens_output <- tcensReg(y ~ 1, data = dt, v = nu) #censored only model, i.e., Tobit model

tcensReg_est <- output$theta #extracting the point estimates
tcensReg_est[2] <- exp(tcensReg_est[2]) #exponentiating the estimate of log_sigma to estimate sigma

lm_est <- c(coef(lm_output), summary(lm_output)$sigma)

cens_est <- cens_output$theta
cens_est[2] <- exp(cens_est[2])

results_df <- data.frame(rbind(c(mu, sigma), t(tcensReg_est), lm_est, t(cens_est)))
names(results_df) <- c("mu", "sigma")
row.names(results_df) <- c("Truth", "tcensReg", "Normal MLE", "Tobit")
results_df$mu_bias <- abs(results_df$mu - mu)
results_df$sigma_bias <- abs(results_df$sigma - sigma)

knitr::kable(results_df, format = "markdown", digits = 4)
```
Other methods result in significant bias for both $\mu$ and $\sigma$.

